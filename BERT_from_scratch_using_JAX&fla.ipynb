{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPkyZvmoG8POENc0ZkiSwiH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zainmo11/BERT_from_scratch_using_JAX-flax/blob/main/BERT_from_scratch_using_JAX%26fla.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install requirement"
      ],
      "metadata": {
        "id": "F020yJ2U6f27"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEUacPynFj7c",
        "outputId": "f3be3a85-d172-44c7-fa2f-7e35adc571d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (0.4.26)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from jax) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax) (1.13.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install jax\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet flax\n",
        "import jax\n",
        "from flax import linen as jnn"
      ],
      "metadata": {
        "id": "oIqL3hQWdaha"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# important Imports"
      ],
      "metadata": {
        "id": "Ivj9y1PK6sBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.nn as jnn\n",
        "import optax\n",
        "import re\n",
        "from random import *\n",
        "import jax.scipy.special as jsp"
      ],
      "metadata": {
        "id": "QfIEFmtZGbSs"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text data example"
      ],
      "metadata": {
        "id": "E6ZaiYcB6wZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    'I enjoy solving complex problems and the satisfaction that comes with finding innovative solutions. There’s something exhilarating about coding, isn’t there? Whether it\\'s debugging a tricky issue or designing a new feature from scratch, the process of problem-solving is incredibly rewarding. I can spend hours immersed in code, trying to understand how different components interact and how to optimize their performance. However, I can\\'t stand inefficiency and poorly designed systems that make even the simplest tasks cumbersome. Dealing with outdated documentation is particularly frustrating, especially when it hinders progress and wastes valuable time.\\n'\n",
        "\n",
        "    'One of the aspects I find most thrilling about programming is the ability to refactor and improve existing code. The challenge of taking something old and making it better, more efficient, or more readable is immensely satisfying. It’s like giving a new lease on life to a piece of software that might otherwise be languishing in obscurity. On the other hand, I dislike dealing with codebases that are full of hacks and workarounds rather than clean, maintainable solutions.\\n'\n",
        "\n",
        "    'Exploring new algorithms and data structures is another aspect of programming that I deeply enjoy. The way different algorithms can be applied to solve various types of problems is fascinating. It’s a never-ending learning experience, as there’s always something new to discover or a different approach to consider. I find that the flexibility and adaptability of algorithms make them incredibly powerful tools in software development. However, I do have a strong aversion to systems that lack proper documentation or are poorly maintained. It can be quite challenging to work with legacy systems that are difficult to understand or modify.\\n'\n",
        "\n",
        "    'Building something from the ground up, whether it\\'s a new application or a piece of infrastructure, is always an exciting challenge. The process of taking an idea and turning it into a working product is both demanding and rewarding. It requires careful planning, design, and execution, and the sense of accomplishment when the project is completed is unparalleled. I find that this sense of achievement is one of the main reasons I continue to be passionate about programming.\\n'\n",
        "\n",
        "    'Despite my enthusiasm, I do encounter frustrations along the way. For instance, when the system crashes unexpectedly or when features are removed without prior notice, it can be quite disheartening. I hate dealing with such disruptions, as they can throw off development schedules and require additional troubleshooting. Compatibility issues are another source of frustration. When different components or systems don’t work well together, it can lead to significant delays and additional work.\\n'\n",
        "\n",
        "    'Integrating APIs and external services into a project can be both a challenge and an opportunity. On one hand, it can be tricky to get everything to work seamlessly together, especially when dealing with inconsistencies or unexpected behavior. On the other hand, it provides an opportunity to extend the functionality of a project and leverage existing tools and services. This balancing act between integration and customization is an essential part of modern software development.\\n'\n",
        "\n",
        "    'Testing is another critical area of focus. While it can sometimes be tedious and time-consuming, it is essential for ensuring that the software works as expected and is free of critical bugs. The process of writing and running tests, analyzing the results, and making necessary adjustments is a fundamental part of the development lifecycle. I often find that the effort put into testing pays off in the long run by preventing issues and improving the overall quality of the software.\\n'\n",
        "\n",
        "    'Collaboration is also an integral part of the development process. Working with other developers, designers, and stakeholders helps to ensure that a project meets its requirements and is aligned with its goals. While collaborating with different teams can be challenging, especially when dealing with diverse opinions and approaches, it also provides valuable opportunities for learning and growth. Effective communication and teamwork are essential for the success of any project, and I appreciate the opportunity to work with talented individuals from various backgrounds.\\n'\n",
        "\n",
        "    'Overall, programming is a field full of challenges and rewards. The constant evolution of technology and the need to stay up-to-date with the latest tools and practices keep the work exciting and dynamic. Despite the occasional frustrations and setbacks, the sense of accomplishment and the opportunity to create meaningful and impactful solutions make it all worthwhile. I look forward to continuing to explore new technologies, tackle complex problems, and contribute to the ever-evolving world of software development. The journey of learning and growing as a developer is something I cherish, and I am excited for the future opportunities and challenges that lie ahead.'\n",
        ")\n"
      ],
      "metadata": {
        "id": "Du3uI4JwHyIz"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3- Data Preprocessing\n",
        "\n",
        "1. Clean Data from .,?!\n",
        "2. Encode our data (ids)."
      ],
      "metadata": {
        "id": "SpipAWIE60pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')  # filter '.', ',', '?', '!'\n",
        "\n",
        "word_list = list(set(\" \".join(sentences).split())) # we make a join for all sentence to be as one sentense then split by \" \" and make it in set to remove redundant words the in a list\n",
        "word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
        "\n",
        "\n",
        "for i, w in enumerate(word_list):\n",
        "    word_dict[w] = i + 4          # For Encoding start from 4\n",
        "\n",
        "number_dict = {i: w for i, w in enumerate(word_dict)} # Create a reverse dictionary for decoding\n",
        "vocab_size = len(word_dict)\n",
        "\n",
        "token_list = list()\n",
        "# Tokenize each sentence into a list of token IDs\n",
        "for sentence in sentences:\n",
        "    arr = [word_dict[s] for s in sentence.split()]\n",
        "    # print(\"arr = \", arr)\n",
        "    token_list.append(arr)\n",
        "\n",
        "print (token_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIk9Ge8dOpU_",
        "outputId": "af78b6dc-2112-4964-d11f-009914c7cb37"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[236, 194, 62, 173, 170, 100, 295, 55, 352, 108, 29, 132, 276, 336, 164, 200, 239, 294, 272, 14, 144, 216, 326, 79, 197, 266, 288, 116, 240, 197, 163, 212, 233, 19, 295, 290, 147, 161, 257, 269, 90, 236, 92, 57, 254, 25, 209, 8, 69, 45, 86, 309, 252, 50, 133, 100, 309, 45, 192, 199, 88, 207, 236, 282, 156, 77, 100, 342, 10, 183, 352, 223, 291, 295, 324, 111, 317, 153, 29, 119, 243, 257, 158, 330, 102, 308, 345, 348, 224, 100, 76, 83, 35], [334, 147, 295, 51, 236, 248, 206, 357, 294, 268, 257, 295, 260, 45, 73, 100, 56, 293, 8, 295, 210, 147, 307, 200, 339, 100, 71, 345, 101, 117, 85, 116, 117, 184, 257, 80, 139, 198, 275, 211, 197, 163, 344, 182, 299, 45, 197, 225, 147, 30, 352, 9, 217, 281, 148, 209, 104, 182, 295, 310, 188, 236, 329, 153, 29, 95, 352, 128, 174, 147, 6, 100, 346, 123, 15, 323, 235, 336], [202, 163, 126, 100, 7, 106, 257, 259, 253, 147, 268, 352, 236, 189, 194, 295, 327, 252, 126, 92, 281, 205, 45, 122, 358, 286, 147, 170, 257, 39, 198, 197, 353, 347, 234, 331, 164, 306, 200, 163, 45, 221, 116, 197, 252, 48, 45, 298, 236, 248, 352, 295, 11, 100, 300, 147, 126, 223, 215, 269, 146, 124, 209, 30, 87, 207, 236, 105, 250, 197, 61, 70, 45, 183, 352, 285, 21, 243, 116, 128, 342, 203, 345, 92, 281, 273, 59, 45, 165, 29, 287, 183, 352, 128, 20, 45, 86, 116, 219], [138, 200, 233, 295, 227, 261, 216, 326, 197, 163, 142, 116, 197, 225, 147, 244, 257, 306, 137, 305, 210, 295, 290, 147, 307, 137, 143, 100, 140, 345, 274, 197, 270, 47, 257, 167, 296, 100, 90, 345, 262, 245, 263, 42, 100, 187, 100, 295, 337, 147, 208, 308, 295, 17, 257, 251, 257, 222, 236, 248, 352, 64, 337, 147, 60, 257, 334, 147, 295, 195, 171, 236, 53, 45, 281, 162, 294, 268], [97, 28, 149, 236, 105, 297, 271, 226, 295, 327, 340, 325, 308, 295, 129, 328, 319, 116, 308, 115, 128, 313, 135, 301, 109, 345, 92, 281, 273, 218, 236, 283, 153, 29, 94, 44, 331, 284, 92, 34, 72, 87, 175, 100, 52, 318, 180, 181, 32, 128, 259, 114, 147, 91, 308, 252, 50, 116, 183, 81, 165, 355, 196, 345, 92, 351, 45, 179, 238, 100, 318, 165], [31, 41, 100, 231, 315, 274, 197, 17, 92, 281, 167, 197, 210, 100, 137, 264, 182, 334, 188, 345, 92, 281, 266, 45, 27, 332, 45, 165, 302, 196, 102, 308, 153, 29, 103, 116, 314, 141, 182, 295, 310, 188, 345, 121, 137, 264, 45, 213, 295, 232, 147, 197, 17, 100, 311, 293, 124, 100, 315, 64, 241, 22, 242, 99, 100, 289, 257, 137, 237, 67, 147, 155, 30, 87], [98, 257, 259, 159, 43, 147, 320, 130, 345, 92, 118, 281, 54, 100, 177, 345, 257, 237, 340, 16, 352, 295, 30, 303, 331, 278, 100, 257, 354, 147, 159, 265, 295, 290, 147, 338, 100, 204, 65, 335, 295, 230, 100, 71, 150, 68, 257, 197, 37, 67, 147, 295, 87, 312, 236, 5, 248, 352, 295, 322, 145, 274, 98, 292, 72, 209, 295, 151, 185, 166, 13, 32, 100, 110, 295, 23, 152, 147, 295, 30], [343, 257, 36, 137, 160, 67, 147, 295, 87, 290, 270, 29, 310, 333, 78, 100, 58, 93, 45, 82, 352, 197, 17, 63, 40, 154, 100, 257, 201, 29, 40, 349, 130, 89, 29, 252, 256, 92, 281, 59, 102, 308, 153, 29, 24, 178, 100, 350, 345, 36, 121, 83, 66, 340, 347, 100, 214, 33, 186, 100, 258, 128, 237, 340, 295, 38, 147, 134, 17, 100, 236, 26, 295, 264, 45, 165, 29, 46, 228, 233, 358, 113], [23, 268, 257, 197, 267, 174, 147, 191, 100, 279, 295, 75, 280, 147, 316, 100, 295, 4, 45, 220, 136, 29, 295, 246, 124, 100, 176, 255, 295, 165, 305, 100, 190, 97, 295, 112, 271, 100, 18, 295, 337, 147, 208, 100, 295, 264, 45, 84, 249, 100, 169, 336, 223, 345, 12, 125, 236, 356, 49, 45, 277, 45, 74, 163, 157, 172, 173, 170, 100, 107, 45, 295, 247, 120, 147, 30, 87, 295, 304, 147, 347, 100, 321, 331, 197, 168, 257, 200, 236, 96, 100, 236, 193, 229, 340, 295, 131, 66, 100, 191, 352, 127, 341]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Prepare our Data for BERT.\n",
        "1. Configure BERT Parameters.\n",
        "2. Make Batches."
      ],
      "metadata": {
        "id": "OL77WBRq68oT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 6\n",
        "maxlen = 30 # maximum of length\n",
        "max_pred = 5  # max tokens of prediction\n",
        "n_layers = 6 # number of Encoder of Encoder Layer\n",
        "n_heads = 12 # number of heads in Multi-Head Attention\n",
        "d_model = 768 # Embedding Size\n",
        "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
        "d_k = d_v = 64  # dimension of K(=Q), V        64*12 head = 768\n",
        "n_segments = 2 # n.o.sentences"
      ],
      "metadata": {
        "id": "QEz2_fWIHyFf"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_batch():\n",
        "    batch = []\n",
        "    positive = negative = 0\n",
        "    while positive != batch_size/2 or negative != batch_size/2:\n",
        "        tokens_a_index, tokens_b_index= randrange(len(sentences)), randrange(len(sentences))  # random index , random index\n",
        "        tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index]  # random sentence , random sentence\n",
        "\n",
        "        input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']] # concatenate random sentences with 'CLS' and 'SEP'\n",
        "\n",
        "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)  # [ 0,0,0,0,0,0,1,1,1,1,1,1,1,1,1]\n",
        "\n",
        "        #MASK LM\n",
        "        n_pred =  min(max_pred, max(1, int(round(len(input_ids) * 0.15)))) # number of masks is up to 15 % of tokens in one sentence\n",
        "\n",
        "        cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
        "                          if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]  # list of indices of input without cls and sep tokens\n",
        "\n",
        "        shuffle(cand_maked_pos)    # shuffle all indices\n",
        "        masked_tokens, masked_pos = [], []\n",
        "        for pos in cand_maked_pos[:n_pred]:\n",
        "            masked_pos.append(pos)     # list of random indices\n",
        "            masked_tokens.append(input_ids[pos]) # list of random tokens\n",
        "            if random() < 0.8:  # 80%\n",
        "                input_ids[pos] = word_dict['[MASK]'] # make mask\n",
        "            elif random() < 0.5:  # 10%\n",
        "                index = randint(0, vocab_size - 1) # random index in vocabulary\n",
        "                input_ids[pos] = word_dict[number_dict[index]] # replace by random word\n",
        "\n",
        "        # Zero Paddings\n",
        "        n_pad = maxlen - len(input_ids) # number of pad tokens we need\n",
        "        input_ids.extend([0] * n_pad)\n",
        "        segment_ids.extend([0] * n_pad)\n",
        "\n",
        "    #     # Zero Padding (100% - 15%) tokens\n",
        "        if max_pred > n_pred: # target 1 , max = 5   [12,0,0,0,0 ]\n",
        "            n_pad = max_pred - n_pred # number of pad tokens in masked list\n",
        "            masked_tokens.extend([0] * n_pad)\n",
        "            masked_pos.extend([0] * n_pad)  # to make masked list is static in shape\n",
        "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
        "            positive += 1\n",
        "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
        "            negative += 1\n",
        "    return batch"
      ],
      "metadata": {
        "id": "eCvxRSviHyDD"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Core Functions\n",
        "1. Get Attention Pad Mask.\n",
        "2. GELU."
      ],
      "metadata": {
        "id": "7_DslLXX7CMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get attention mask\n",
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "    batch_size, len_q = seq_q.shape\n",
        "    _, len_k = seq_k.shape\n",
        "    # Create padding mask where 0 is the padding\n",
        "    pad_attn_mask = (seq_k == 0).astype(jnp.float32)\n",
        "    # Expand dimensions for broadcasting\n",
        "    pad_attn_mask = pad_attn_mask[:, jnp.newaxis, :]\n",
        "    pad_attn_mask = jnp.broadcast_to(pad_attn_mask, (batch_size, len_q, len_k))\n",
        "    return pad_attn_mask\n"
      ],
      "metadata": {
        "id": "AnGVbm4qHxts"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gelu(x):\n",
        "    return x * 0.5 * (1.0 + jsp.erf(x / jnp.sqrt(2.0)))"
      ],
      "metadata": {
        "id": "bHJSSdmaOyXj"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = make_batch()\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxwIvx9nOyP_",
        "outputId": "22083d74-00e4-4653-d3da-d6da94eefc72"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[1, 97, 28, 149, 236, 105, 297, 271, 226, 295, 327, 340, 325, 308, 295, 129, 328, 3, 116, 308, 115, 128, 313, 135, 301, 109, 345, 92, 281, 273, 218, 236, 283, 153, 29, 3, 44, 331, 284, 92, 34, 72, 87, 175, 100, 52, 318, 180, 181, 32, 128, 259, 114, 147, 91, 308, 3, 50, 116, 183, 81, 165, 355, 196, 345, 92, 351, 45, 179, 238, 100, 318, 165, 2, 23, 268, 257, 197, 267, 174, 147, 191, 100, 279, 295, 75, 280, 147, 316, 100, 295, 4, 45, 220, 136, 29, 295, 246, 124, 100, 176, 255, 295, 165, 305, 100, 190, 97, 295, 112, 271, 100, 18, 295, 337, 147, 208, 100, 295, 264, 45, 84, 249, 100, 169, 336, 223, 345, 12, 125, 236, 356, 49, 45, 277, 45, 74, 163, 157, 172, 173, 170, 100, 107, 45, 295, 247, 120, 147, 30, 87, 295, 304, 147, 347, 100, 321, 331, 197, 168, 257, 200, 236, 3, 100, 236, 193, 229, 340, 295, 131, 66, 100, 191, 352, 127, 341, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [252, 94, 319, 96, 218], [56, 35, 17, 163, 30], False], [[1, 202, 3, 126, 100, 7, 3, 257, 259, 253, 147, 268, 352, 236, 189, 194, 295, 327, 252, 126, 92, 281, 205, 45, 122, 358, 286, 147, 170, 257, 39, 198, 197, 353, 347, 234, 3, 164, 306, 200, 163, 45, 221, 116, 197, 252, 48, 45, 298, 236, 248, 352, 295, 11, 100, 300, 147, 126, 223, 215, 269, 146, 124, 209, 30, 87, 207, 236, 105, 250, 197, 61, 70, 45, 183, 352, 285, 21, 243, 116, 128, 342, 203, 345, 92, 281, 273, 59, 45, 165, 29, 287, 183, 352, 128, 20, 45, 86, 116, 219, 2, 98, 257, 259, 159, 43, 147, 320, 130, 345, 92, 118, 281, 54, 100, 177, 345, 257, 237, 340, 16, 352, 295, 30, 303, 3, 278, 100, 257, 354, 147, 159, 265, 295, 290, 147, 338, 100, 204, 65, 335, 295, 230, 100, 71, 150, 68, 257, 197, 37, 67, 147, 295, 87, 312, 236, 5, 248, 352, 295, 322, 145, 3, 98, 292, 72, 209, 295, 151, 185, 166, 13, 32, 100, 110, 295, 23, 152, 147, 295, 30, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [331, 163, 274, 106, 331], [36, 2, 162, 6, 125], False], [[1, 138, 200, 3, 295, 227, 261, 216, 326, 197, 163, 142, 116, 197, 225, 147, 244, 257, 306, 137, 305, 210, 295, 290, 147, 307, 137, 143, 100, 140, 345, 274, 197, 270, 47, 257, 167, 296, 100, 90, 345, 262, 245, 263, 42, 100, 187, 100, 295, 337, 147, 208, 308, 295, 17, 257, 251, 257, 222, 236, 248, 352, 64, 337, 147, 60, 257, 334, 147, 295, 195, 171, 236, 53, 45, 281, 162, 294, 268, 2, 31, 41, 100, 231, 315, 274, 197, 17, 92, 281, 167, 197, 210, 100, 137, 264, 182, 334, 188, 345, 92, 281, 266, 45, 27, 332, 45, 3, 302, 196, 102, 308, 153, 29, 3, 116, 314, 141, 182, 295, 310, 188, 345, 121, 137, 264, 45, 213, 295, 232, 147, 197, 17, 100, 311, 293, 325, 100, 315, 64, 241, 22, 242, 99, 100, 289, 257, 137, 237, 67, 147, 155, 30, 87, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [165, 296, 233, 103, 124], [107, 37, 3, 114, 136], False], [[1, 236, 194, 62, 173, 170, 100, 295, 55, 352, 108, 29, 132, 276, 336, 164, 200, 239, 294, 272, 14, 144, 216, 326, 79, 197, 266, 288, 116, 240, 197, 163, 212, 233, 19, 295, 290, 147, 161, 257, 269, 90, 236, 92, 57, 254, 25, 209, 8, 69, 45, 86, 309, 252, 50, 133, 100, 309, 45, 192, 199, 88, 207, 236, 282, 156, 77, 100, 342, 10, 183, 352, 223, 291, 295, 324, 111, 317, 153, 29, 119, 243, 257, 158, 330, 102, 3, 345, 348, 224, 3, 76, 83, 35, 2, 334, 147, 295, 51, 236, 248, 206, 357, 294, 268, 257, 295, 260, 45, 73, 100, 56, 293, 8, 295, 210, 147, 307, 200, 339, 100, 3, 345, 101, 117, 85, 116, 117, 184, 257, 80, 139, 198, 275, 211, 197, 163, 344, 3, 299, 45, 197, 225, 147, 30, 3, 9, 217, 281, 148, 209, 104, 182, 295, 310, 188, 236, 329, 153, 29, 95, 352, 128, 174, 147, 6, 100, 346, 123, 15, 323, 235, 336, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [182, 352, 71, 100, 308], [138, 145, 121, 90, 86], True], [[1, 236, 194, 62, 173, 170, 100, 295, 55, 352, 108, 29, 132, 276, 336, 164, 200, 239, 294, 272, 14, 144, 216, 326, 79, 197, 266, 288, 1, 240, 197, 163, 212, 233, 19, 295, 290, 147, 161, 257, 269, 90, 236, 92, 57, 254, 25, 209, 8, 69, 45, 86, 309, 252, 50, 133, 100, 309, 45, 192, 199, 88, 207, 236, 282, 156, 77, 100, 342, 10, 183, 3, 223, 291, 295, 324, 111, 317, 153, 29, 119, 243, 257, 158, 330, 102, 308, 345, 348, 224, 100, 76, 83, 35, 2, 334, 147, 295, 51, 236, 248, 206, 357, 294, 268, 257, 295, 260, 45, 73, 100, 56, 293, 8, 295, 210, 147, 307, 200, 339, 100, 71, 345, 101, 117, 85, 3, 117, 184, 257, 80, 139, 198, 275, 211, 197, 163, 344, 182, 299, 45, 197, 225, 147, 30, 352, 9, 217, 281, 148, 209, 104, 182, 295, 310, 188, 236, 329, 153, 29, 95, 352, 128, 174, 147, 6, 100, 346, 123, 15, 323, 235, 336, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [116, 25, 309, 352, 116], [28, 46, 52, 71, 126], True], [[1, 138, 200, 233, 295, 227, 261, 216, 326, 197, 163, 3, 116, 197, 225, 147, 244, 257, 306, 137, 305, 3, 295, 290, 147, 307, 137, 334, 100, 140, 345, 274, 197, 270, 47, 257, 167, 296, 100, 90, 345, 262, 245, 263, 42, 100, 187, 100, 295, 337, 147, 208, 308, 3, 17, 257, 251, 257, 222, 236, 248, 352, 64, 337, 147, 60, 257, 334, 147, 295, 195, 171, 236, 53, 45, 281, 162, 294, 268, 2, 97, 28, 149, 236, 105, 297, 271, 226, 295, 327, 340, 325, 308, 295, 129, 328, 319, 116, 308, 115, 128, 313, 135, 301, 109, 345, 92, 281, 273, 218, 3, 283, 153, 29, 94, 44, 331, 284, 92, 34, 72, 87, 175, 100, 52, 318, 180, 181, 32, 128, 259, 114, 147, 91, 308, 252, 50, 116, 183, 81, 165, 355, 196, 345, 92, 351, 45, 179, 238, 100, 318, 165, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [143, 210, 142, 295, 236], [27, 21, 11, 53, 110], True]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to pad sequences\n",
        "def pad_sequence(seq, max_length):\n",
        "    return seq + [0] * (max_length - len(seq))\n",
        "\n",
        "# Determine max length for padding\n",
        "max_length = max([max(len(seq) for seq in item[:4] if isinstance(seq, list)) for item in batch])\n",
        "\n",
        "# Processing the batch and converting to JAX tensors\n",
        "input_ids = jnp.array([pad_sequence(item[0], max_length) for item in batch], dtype=jnp.int32)\n",
        "segment_ids = jnp.array([pad_sequence(item[1], max_length) for item in batch], dtype=jnp.int32)\n",
        "masked_tokens = jnp.array([pad_sequence(item[2], max_length) for item in batch], dtype=jnp.int32)\n",
        "masked_pos = jnp.array([pad_sequence(item[3], max_length) for item in batch], dtype=jnp.int32)\n",
        "isNext = jnp.array([item[4] for item in batch], dtype=jnp.bool_)"
      ],
      "metadata": {
        "id": "fjjgiWDKP-kd"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the attention mask for the batch\n",
        "attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
        "\n",
        "# Output for debugging\n",
        "print(\"Input IDs:\\n\", input_ids)\n",
        "print(\"Segment IDs:\\n\", segment_ids)\n",
        "print(\"Masked Tokens:\\n\", masked_tokens)\n",
        "print(\"Masked Positions:\\n\", masked_pos)\n",
        "print(\"Is Next:\\n\", isNext)\n",
        "print(\"Attention Mask:\\n\", attn_mask[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZeMq8w0QC99",
        "outputId": "e6fd85ed-d1a1-4218-9a47-fd8b438399f8"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs:\n",
            " [[  1  97  28 ...   0   0   0]\n",
            " [  1 202   3 ... 295  30   2]\n",
            " [  1 138 200 ...   0   0   0]\n",
            " [  1 236 194 ...   0   0   0]\n",
            " [  1 236 194 ...   0   0   0]\n",
            " [  1 138 200 ...   0   0   0]]\n",
            "Segment IDs:\n",
            " [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 1 1 1]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "Masked Tokens:\n",
            " [[252  94 319 ...   0   0   0]\n",
            " [331 163 274 ...   0   0   0]\n",
            " [165 296 233 ...   0   0   0]\n",
            " [182 352  71 ...   0   0   0]\n",
            " [116  25 309 ...   0   0   0]\n",
            " [143 210 142 ...   0   0   0]]\n",
            "Masked Positions:\n",
            " [[ 56  35  17 ...   0   0   0]\n",
            " [ 36   2 162 ...   0   0   0]\n",
            " [107  37   3 ...   0   0   0]\n",
            " [138 145 121 ...   0   0   0]\n",
            " [ 28  46  52 ...   0   0   0]\n",
            " [ 27  21  11 ...   0   0   0]]\n",
            "Is Next:\n",
            " [False False False  True  True  True]\n",
            "Attention Mask:\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Building Blocks\n",
        "1. Embedding.\n",
        "2. Multi Head Attention.\n",
        "3. Feed Forward\n",
        "4. Encoder Block"
      ],
      "metadata": {
        "id": "3XR2kO8J7GZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "import flax.linen as nn\n",
        "from jax import random\n",
        "import numpy as np\n",
        "# Initialize PRNG key\n",
        "rng = random.PRNGKey(0)\n",
        "class Embedding(nn.Module):\n",
        "    vocab_size: int\n",
        "    d_model: int\n",
        "    maxlen: int\n",
        "    n_segments: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, input_ids, segment_ids):\n",
        "        tok_embed = nn.Embed(self.vocab_size, self.d_model)(input_ids)\n",
        "        pos_embed = nn.Embed(self.maxlen, self.d_model)(jnp.arange(input_ids.shape[1])[None, :])\n",
        "        seg_embed = nn.Embed(self.n_segments, self.d_model)(segment_ids)\n",
        "        embedding = tok_embed + pos_embed + seg_embed\n",
        "        return nn.LayerNorm()(embedding)"
      ],
      "metadata": {
        "id": "Xa2lK3DAQC3c"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    d_k: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, Q, K, V, attn_mask):\n",
        "        scores = jnp.einsum('bqhd,bkhd->bhqk', Q, K) / np.sqrt(self.d_k)\n",
        "        scores = jnp.where(attn_mask == 0, scores, -1e9)  # Apply the attention mask\n",
        "        attn = nn.softmax(scores, axis=-1)\n",
        "        context = jnp.einsum('bhqk,bkhd->bqhd', attn, V)\n",
        "        return context, attn"
      ],
      "metadata": {
        "id": "5G9qZFm1TrKL"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Flax RNG\n",
        "rng = random.PRNGKey(0)\n",
        "\n",
        "# Example usage (input_ids, segment_ids need to be provided)\n",
        "embedding_layer = Embedding(vocab_size=50000, d_model=512, maxlen=512, n_segments=2)\n",
        "\n",
        "# Initialize the parameters\n",
        "params = embedding_layer.init(rng, input_ids, segment_ids)\n",
        "\n",
        "# Use the initialized parameters\n",
        "embeds = embedding_layer.apply(params, input_ids, segment_ids)\n",
        "\n",
        "attenM = get_attn_pad_mask(input_ids, input_ids)\n",
        "\n",
        "attention_layer = ScaledDotProductAttention(d_k=64)\n",
        "context, attn = attention_layer.apply({'params': {}}, embeds, embeds, embeds, attenM)\n",
        "\n",
        "print('Masks:', attenM[0][0])\n",
        "print()\n",
        "print('Attention Scores after softmax: ', attn[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "KVXB1QaDTs5l",
        "outputId": "58ebcc48-67ae-474d-b3e5-42b8c050ab89"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Einstein sum subscript 'bqhd' does not contain the correct number of indices for operand 0.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-62a46348f172>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mattention_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScaledDotProductAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattenM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Masks:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattenM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-8f80e615c61d>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, Q, K, V, attn_mask)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bqhd,bkhd->bhqk'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_mask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1e9\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply the attention mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(subscripts, out, optimize, precision, preferred_element_type, _dot_general, *operands)\u001b[0m\n\u001b[1;32m   3567\u001b[0m     \u001b[0mcontract_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poly_einsum_handlers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_default_poly_einsum_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3568\u001b[0m   \u001b[0;31m# using einsum_call=True here is an internal api for opt_einsum... sorry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3569\u001b[0;31m   operands, contractions = contract_path(\n\u001b[0m\u001b[1;32m   3570\u001b[0m         *operands, einsum_call=True, use_blas=True, optimize=optimize)\n\u001b[1;32m   3571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/opt_einsum/contract.py\u001b[0m in \u001b[0;36mcontract_path\u001b[0;34m(*operands, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             raise ValueError(\"Einstein sum subscript '{}' does not contain the \"\n\u001b[0m\u001b[1;32m    229\u001b[0m                              \"correct number of indices for operand {}.\".format(input_list[tnum], tnum))\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Einstein sum subscript 'bqhd' does not contain the correct number of indices for operand 0."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import flax.linen as nn\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    d_model: int\n",
        "    d_k: int\n",
        "    d_v: int\n",
        "    n_heads: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, Q, K, V, attn_mask):\n",
        "        batch_size = Q.shape[0]\n",
        "        q_s = nn.Dense(self.d_k * self.n_heads)(Q).reshape(batch_size, -1, self.n_heads, self.d_k)\n",
        "        k_s = nn.Dense(self.d_k * self.n_heads)(K).reshape(batch_size, -1, self.n_heads, self.d_k)\n",
        "        v_s = nn.Dense(self.d_v * self.n_heads)(V).reshape(batch_size, -1, self.n_heads, self.d_v)\n",
        "\n",
        "        context, attn = ScaledDotProductAttention(self.d_k)(q_s, k_s, v_s, attn_mask)\n",
        "        context = context.reshape(batch_size, -1, self.d_v * self.n_heads)\n",
        "        output = nn.Dense(self.d_model)(context)\n",
        "        return nn.LayerNorm()(output + Q), attn"
      ],
      "metadata": {
        "id": "mrfTneIVTq9D"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model instantiation\n",
        "embedding_layer = Embedding(vocab_size=50000, d_model=512, maxlen=512, n_segments=2)\n",
        "multihead_attention_layer = MultiHeadAttention(d_model=512, d_k=64, d_v=64, n_heads=8)\n",
        "\n",
        "# Initialize parameters for Embedding\n",
        "params_emb = embedding_layer.init(rng, input_ids, segment_ids)\n",
        "# Apply embedding\n",
        "embeds = embedding_layer.apply(params_emb, input_ids, segment_ids)\n",
        "\n",
        "# Initialize parameters for MultiHeadAttention\n",
        "params_mha = multihead_attention_layer.init(rng, embeds, embeds, embeds, attn_mask)\n",
        "\n",
        "# Apply MultiHeadAttention\n",
        "output, attn = multihead_attention_layer.apply(params_mha, embeds, embeds, embeds, attn_mask)\n",
        "\n",
        "print(\"Output:\", output)\n",
        "print(\"Attention Weights:\", attn)"
      ],
      "metadata": {
        "id": "Yj0efwd6VInB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "outputId": "abe331bd-ec1d-4c4a-c894-6cde1b8788da"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Incompatible shapes for broadcasting: shapes=[(6, 182, 182), (2, 8, 3, 3), ()]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/util.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/util.py\u001b[0m in \u001b[0;36mcached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_broadcast_shapes_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_broadcast_shapes_uncached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mresult_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Incompatible shapes for broadcasting: shapes={list(shapes)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(6, 182, 182), (2, 8, 3, 3), ()]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-46fb9e89ba17>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Initialize parameters for MultiHeadAttention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mparams_mha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultihead_attention_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Apply MultiHeadAttention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-89-98578e0e1180>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, Q, K, V, attn_mask)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mv_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_v\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScaledDotProductAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_v\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-8f80e615c61d>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, Q, K, V, attn_mask)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bqhd,bkhd->bhqk'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_mask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1e9\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply the attention mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bhqk,bkhd->bqhd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(acondition, if_true, if_false, size, fill_value, condition, x, y)\u001b[0m\n\u001b[1;32m   1146\u001b[0m                        \u001b[0;34m\"should be provided to jax.numpy.where, got \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m                        f\"{if_true} and {if_false}.\")\n\u001b[0;32m-> 1148\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_where\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/util.py\u001b[0m in \u001b[0;36m_where\u001b[0;34m(condition, x, y)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0mcondition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m   \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpromote_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m   \u001b[0mcondition_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_broadcast_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[0mis_always_empty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_empty_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/util.py\u001b[0m in \u001b[0;36m_broadcast_arrays\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    405\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefinitely_equal_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m   \u001b[0mresult_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_broadcast_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0mresult_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_broadcast_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mresult_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Incompatible shapes for broadcasting: shapes={list(shapes)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(6, 182, 182), (2, 8, 3, 3), ()]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(A[0][0])"
      ],
      "metadata": {
        "id": "vS06pfM7VIg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PoswiseFeedForwardNet(jnn.Module):\n",
        "    def __init__(self):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.fc1 = jnn.Linear(d_model, d_ff)\n",
        "        self.fc2 = jnn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
        "        return self.fc2(gelu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "_LpZ92blVPsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(jnn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention()\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
        "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
        "        return enc_outputs, attn"
      ],
      "metadata": {
        "id": "feoAsBlkVPqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. BERT"
      ],
      "metadata": {
        "id": "HVjgSaFQ7MA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT(jnn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERT, self).__init__()\n",
        "        self.embedding = Embedding()\n",
        "        self.layers = jnn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
        "        self.fc = jnn.Linear(d_model, d_model)\n",
        "        self.activ1 = jnn.Tanh()\n",
        "        self.linear = jnn.Linear(d_model, d_model)\n",
        "        self.activ2 = gelu\n",
        "        self.norm = jnn.LayerNorm(d_model)\n",
        "        self.classifier = jnn.Linear(d_model, 2)\n",
        "        # decoder is shared with embedding layer\n",
        "        embed_weight = self.embedding.tok_embed.weight\n",
        "        n_vocab, n_dim = embed_weight.size()\n",
        "        self.decoder = jnn.Linear(n_dim, n_vocab, bias=False)\n",
        "        self.decoder.weight = embed_weight\n",
        "        self.decoder_bias = jnn.Parameter(jax.zeros(n_vocab))\n",
        "\n",
        "    def forward(self, input_ids, segment_ids, masked_pos):\n",
        "        output = self.embedding(input_ids, segment_ids)\n",
        "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
        "        for layer in self.layers:\n",
        "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
        "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_model, d_model]\n",
        "        # it will be decided by first token(CLS)\n",
        "        h_pooled = self.activ1(self.fc(output[:, 0])) # [batch_size, d_model]\n",
        "        logits_clsf = self.classifier(h_pooled) # [batch_size, 2]\n",
        "\n",
        "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
        "        # get masked position from final output of transformer.\n",
        "        h_masked = jax.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
        "        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n",
        "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
        "\n",
        "        return logits_lm, logits_clsf"
      ],
      "metadata": {
        "id": "9FYMpSRuVPoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Train Our Model"
      ],
      "metadata": {
        "id": "XC-lpkYN7Phh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERT()\n",
        "criterion = jnn.CrossEntropyLoss()\n",
        "optimizer = optax.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "batch = make_batch()\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(jax.LongTensor, zip(*batch))\n",
        "\n",
        "for epoch in range(500):\n",
        "    optimizer.zero_grad()\n",
        "    logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
        "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
        "    loss_lm = (loss_lm.float()).mean()\n",
        "    loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n",
        "    loss = loss_lm + loss_clsf\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "VsQd2EQBVPl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Inference"
      ],
      "metadata": {
        "id": "9aX1nVLX7SA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict mask tokens ans isNext\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(jax.LongTensor, zip(batch[0]))\n",
        "print(text)\n",
        "print([number_dict[w.item()] for w in input_ids[0] if number_dict[w.item()] != '[PAD]'])\n",
        "\n",
        "logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
        "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
        "print('masked tokens list : ',[pos.item() for pos in masked_tokens[0] if pos.item() != 0])\n",
        "print('predict masked tokens list : ',[pos for pos in logits_lm if pos != 0])\n",
        "\n",
        "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
        "print('isNext : ', True if isNext else False)\n",
        "print('predict isNext : ',True if logits_clsf else False)"
      ],
      "metadata": {
        "id": "2ju1BBMIVPj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "02yNa_RAVPhb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}